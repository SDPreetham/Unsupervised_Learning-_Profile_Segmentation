# -*- coding: utf-8 -*-
"""Profile_Clustering.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PHSioZZDpLxwxY-1KA7c5nBLkPOEU870

## **Unsupervised Segmentation: Clustering Scaler Learners by Job and Company Features**

- **By S D Preetham**

**Problem Description:**

* Scaler, an online upskilling platform, aims to identify top companies and job positions for its learners by analyzing their professional profiles. As a part of this objective, clustering of learners is done,based on features like job role, company, compensation (CTC) and experience, uncovering patterns among similar learner segments.

* This analysis helps Scaler profile high-performing roles and companies, provide targeted career recommendations and tailor its programs to better align with industry trends and learner aspirations.
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('scaler.csv')
df.head()

df = df.drop(columns = ['Unnamed: 0'])

df.head()

"""### **Exploratory Data Analyis:**"""

print('The number of rows(learners) are: ', df.shape[0])
print('The number of columns are: ', df.shape[1])

df.info()

len(df[df.duplicated()])

df = df.drop_duplicates().reset_index(drop=True)

df.duplicated().sum()

print('The number of rows(learners) are: ', df.shape[0])
print('The number of columns are: ', df.shape[1])

df.isnull().sum()

df.nunique()

"""**Insights:**

* The dataset originally consists of 2,05,843 rows and 6 columns, out of which 34 have been found to be duplicated. After the removal of duplicate rows, the dataset has a size of 2,05,809 rows and 6 columns.

* Although the dataset consists of 2,05,843 rows, there are only 1,53,443 unique learners. This may be due to the switch made by learners to a different roles/companies during their learning at Scaler, thus making these changes reflect using multiple rows.

* The dataset also consists of 52548 null values in job position column, 86 null values in org year column and 44 missing company hashes. This signals a requirement for imputation.

### **Data Preprocessing:**
"""

import re
df['job_position'] = df['job_position'].astype(str).apply(lambda x: re.sub('[^A-Za-z0-9 ]+', '', x).strip())

"""**Explanation:**

* This code removes any characters from the job_position column that are not letters, numbers or spaces and then removes any extra spaces at the beginning or end of the cleaned string. This helps standardize the job position entries for further analysis.

**Handling null values:**
"""

from sklearn.impute import KNNImputer

# Replacing the missing company values with the mode
df['company_hash'] = df['company_hash'].fillna(df['company_hash'].mode()[0])

# Imputation of missing org_year values using K-NN Imputer
imputer = KNNImputer(n_neighbors = 5)
df[['orgyear']] = imputer.fit_transform(df[['orgyear']])
df['orgyear'] = df['orgyear'].round().astype(int)

import numpy as np

df['job_position'] = df['job_position'].replace('nan', np.nan)

# Imputing missing job positions by group-wise mode (company_hash)
df['job_position'] = df.groupby('company_hash')['job_position'].transform(
    lambda x: x.fillna(x.mode().iloc[0]) if not x.mode().empty else x)

# Imputing remaining missing values (those in all-NaN groups) with overall mode
df['job_position'] = df['job_position'].fillna(df['job_position'].mode()[0])

df.isnull().sum()

"""**Computing years of experience:**"""

from datetime import datetime
current = datetime.now().year
df['years_experience'] = current - df['orgyear']
df.head()

"""### **Univariate Analysis:**

**Distribution plots for continuous variables:**
"""

df_filtered = df[
    (df['ctc'].between(df['ctc'].quantile(0.01), df['ctc'].quantile(0.99))) &
    (df['years_experience'].between(df['years_experience'].quantile(0.01), df['years_experience'].quantile(0.99)))
]

continuous = ['ctc', 'years_experience']

plt.figure(figsize=(14, 6))

for i, col in enumerate(continuous):
    plt.subplot(1, 2, i + 1)
    sns.histplot(df_filtered[col], kde=True, bins=40)
    plt.title(f'Distribution of {col}')
    plt.xlabel(col)
    plt.ylabel('Frequency')

plt.tight_layout()
plt.show()

"""**Insights:**

* CTC Distribution:
 * This is a highly right skewed distribution.
 * A majority of learners at Scaler earn between Rs 10-14 lakhs per annum.
* Experience in years Distribution:
 * A large number of learners at Scaler have an experience of 6-8 years. This suggests that the dataset predominantly consists of professionals in the mid-career phase.
"""

# Countplot for ctc_updated_year
sns.countplot(data=df, x='ctc_updated_year', color='darkgreen')
plt.title('Distribution of CTC Updated Year', fontsize=14)
plt.xlabel('CTC Updated Year')
plt.ylabel('Number of Learners')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

df['job_position'] = df['job_position'].str.strip().str.title()

filtered_df = df[df['job_position'].str.lower() != 'unknown']

top_jobs = filtered_df['job_position'].value_counts().head(20)

plt.figure(figsize=(14, 7))
sns.set_style("whitegrid")
sns.barplot(x=top_jobs.values, y=top_jobs.index, palette='viridis')
plt.title('Top-20 Most Common Job Positions Among Learners', fontsize=16)
plt.xlabel('Number of Learners')
plt.ylabel('Job Position')
plt.grid(axis='x', linestyle='--', alpha=0.7)

plt.tight_layout()
plt.show()

# Calculate percentage distribution of job positions
job_percent = (filtered_df['job_position'].value_counts(normalize=True) * 100).round(2)
job_percent_df = job_percent.reset_index()
job_percent_df.columns = ['Job Position', 'Percentage']

print(job_percent_df.head(20))

"""**Insights:**
* It is observed that over 50% of learners at Scaler are into the field of Web Development, with Backend Engineers and FullStack Engineers contributing to ~38% and ~15% respectively.
* The learners work in diverse areas such as Data Science, Devops and Product Management etc.

"""

# Get top 10 most frequent companies
top_companies = df['company_hash'].value_counts().nlargest(10).index
company_mapping = {hash_val: f'Company {i+1}' for i, hash_val in enumerate(top_companies)}
df['company_label'] = df['company_hash'].map(company_mapping)
filtered_df = df[df['company_label'].notnull()]
plt.figure(figsize=(10, 6))
sns.set_style('whitegrid')

sns.countplot(data=filtered_df, y='company_label', order=filtered_df['company_label'].value_counts().index, palette='crest')

plt.title('Top 10 Companies by Number of Learners')
plt.xlabel('Number of Learners')
plt.ylabel('Company')
plt.tight_layout()
plt.show()

"""**Insight:**

* It is observed that over 8000 learners at Scaler work for a same company.
* Similarly, there are few such other companies where the Scaler Learners work.

### **Bivariate Analysis:**

**Average CTC by Job Position:**
"""

plt.figure(figsize=(12, 6))
top_jobs = df[df['job_position'] != 'Unknown']['job_position'].value_counts().nlargest(20).index

sns.barplot(data=df[df['job_position'].isin(top_jobs)],
            x='job_position', y='ctc',
            estimator=np.mean, palette='coolwarm')
plt.xticks(rotation=45)
plt.title('Average CTC by Top 10 Job Positions (Excluding Unknown)')
plt.ylabel('Average CTC in millions')
plt.xlabel('Job Position')
plt.tight_layout()

"""**Insights:**
* It is observed that the average CTC of most of the learners ranges between Rs 1.5-2 million per annum.
* Average CTC of learners who work in the field of Product Management and Data Analysis is observed to be over 4 million rupees per annum.
* Some of the other high paying fields include Engineering Leadership and Program Management, with an average CTC of nearly 3.8 million rupees per annum.
"""

plt.figure(figsize=(10, 6))
sns.scatterplot(data=df, x='years_experience', y='ctc', alpha=0.5)
plt.xlim(0,35)
plt.title('CTC vs Years of Experience')
plt.xlabel('Years of Experience')
plt.ylabel('CTC (in millions)')
plt.grid(True)
plt.tight_layout()
plt.show()

"""**Insights:**

* It is observed that there is a proportional relationship between CTC and Years of Experience between 0-5 Years. However, the CTC remains unaffected after this stage.

### **Feature Engineering:**
"""

df['company_hash'] = df['company_hash'].apply(lambda x: re.sub(r'[^A-Za-z0-9 ]+', '', str(x)))

"""**Standardization and Encoding:**"""

df.drop(columns = ['company_label'], inplace = True)

df.head()

# Scaling the numerical columns in the dataset using Standard Scaler technique

from sklearn.preprocessing import StandardScaler

clustering_df = df[['ctc', 'years_experience']].copy()

scaler = StandardScaler()
clustering_scaled = scaler.fit_transform(clustering_df)
clustering_df_scaled = pd.DataFrame(clustering_scaled, columns=['ctc_scaled', 'years_experience_scaled'])
clustering_df_scaled.head()

# Encoding the required categorical features using Label Encoding technique

from sklearn.preprocessing import LabelEncoder

le_company = LabelEncoder()
le_job = LabelEncoder()

df['company_encoded'] = le_company.fit_transform(df['company_hash'])
df['job_encoded'] = le_job.fit_transform(df['job_position'])

"""### **Manual Clustering:**"""

# Getting the 5 point summary of CTC (mean, median, max, min, count etc) on the basis of Company, Job Position and Years of Experience

summary = df.groupby(['company_hash', 'job_position', 'years_experience'])['ctc'].agg(['mean','median','max','min','count']).reset_index()
summary.columns = ['company_hash','job_position','years_experience','avg_ctc','med_ctc','max_ctc','min_ctc','count']

df = df.merge(summary, on = ['company_hash', 'job_position', 'years_experience'], how  = 'left')
df['designation'] = df.apply(
    lambda x: 1 if x['ctc'] > x['avg_ctc'] else (3 if x['ctc'] < x['avg_ctc'] else 2),
    axis=1
)

"""**Explanation:**
* This code block calculates the 5-point summary (mean, median, max, min and count) of the 'ctc' for each unique combination of 'company_hash', 'job_position' and 'years_experience'. It then merges this summary back into the main DataFrame df and creates a new column 'designation' that categorizes each individual's CTC relative to the average CTC for their specific group.
* This helps to understand how an individual's salary when compared to others in the same company, job and experience level.
"""

# Average CTC for each unique combination of company and job position
class_group = df.groupby(['company_hash', 'job_position'])['ctc'].mean().reset_index()
class_group.columns = ['company_hash', 'job_position', 'avg_class_ctc']

df = df.merge(class_group, on = ['company_hash', 'job_position'])
df['Class'] = df.apply(lambda x:1 if x.ctc > x.avg_class_ctc else (3 if x.ctc< x.avg_class_ctc else 2), axis = 1)

"""**Explanation:**

* This code block calculates the average CTC for each unique combination of company and job position. It then merges this average back into the main DataFrame and creates a new column called 'Class'. The 'Class' column categorizes each individual's CTC based on whether it's above, below or equal to the average CTC for their specific company and job position. This helps to understand how an individual's salary compares to others in the same company and job role.
"""

# Average CTC for each unique company
tier_group = df.groupby(['company_hash'])['ctc'].mean().reset_index()
tier_group.columns = ['company_hash', 'avg_tier_ctc']
df = df.merge(tier_group, on ='company_hash')
df['Tier'] = df.apply(lambda x: 1 if x.ctc> x.avg_tier_ctc else ( 3 if x.ctc < x.avg_tier_ctc else 2), axis = 1)

"""**Explanation:**
* This code block calculates the average CTC for each unique company and then merges this average back into the main DataFrame. It then creates a new column called 'Tier' that categorizes each individual's CTC based on whether it's above, below or equal to the average CTC for their specific company.

### **Deriving Insights from Manual Clustering:**
"""

plt.figure(figsize=(6, 4))
sns.countplot(data=df, x='Tier', palette='coolwarm')
plt.title('Distribution of Learners by Tier Flag')
plt.xlabel('Tier')
plt.ylabel('Number of Learners')
plt.grid(axis='y')
plt.show()

# Top-10 Employees (Highest CTC)
top10_tier1 = df[df['Tier'] == 1].sort_values(by='ctc', ascending=False).head(10)
top10_tier1

# Top-10 Data Science Employees in Each Company

top10_class1_ds = df[(df['job_position'].str.contains('data science', case=False)) & (df['Class'] == 1)]
top10_class1_ds = top10_class1_ds.groupby('company_hash').apply(lambda x: x.sort_values('ctc', ascending=False).head(10)).reset_index(drop=True)

top10_companies = df.groupby('company_hash')['ctc'].mean().sort_values(ascending=False).head(10)

# Bottom-10 Data Science Employees in Each Company
bottom10_class3_ds = df[(df['job_position'].str.contains('data science', case=False)) & (df['Class'] == 3)]
bottom10_class3_ds = bottom10_class3_ds.groupby('company_hash').apply(lambda x: x.sort_values('ctc').head(10)).reset_index(drop=True)

# Bottom-10 Employees by CTC (Overall)
bottom10_tier3 = df[df['Tier'] == 3].sort_values(by='ctc').head(10)

# Top 10 employees in each company in Data Science department having 5-7 years of experience earning more than their peers
df_filter = df[(df['years_experience'] == 5) &
               (df['job_position'].str.contains('data science', case=False)) &
               (df['Tier'] == 1)]

top10_exp5_ds = df_filter.groupby('company_hash').apply(lambda x: x.sort_values('ctc', ascending=False).head(10)).reset_index(drop=True)

# Top 10 Companies by Average CTC
top10_companies = df.groupby('company_hash')['ctc'].mean().sort_values(ascending=False).head(10)
top10_companies

# Top 2 Job Positions per Company (based on CTC)
top_positions = df.groupby(['company_hash', 'job_position'])['ctc'].mean().reset_index()
top_positions = top_positions.sort_values(['company_hash', 'ctc'], ascending=[True, False])
top_positions = top_positions.groupby('company_hash').head(2)

"""### **Unsupervised learning - K-Means Clustering:**"""

encoded_features = df[['job_encoded', 'company_encoded']]
encoded_scaled = StandardScaler().fit_transform(encoded_features)

clustering_features = pd.concat([clustering_df_scaled, df[['job_encoded', 'company_encoded']]], axis = 1)

from sklearn.cluster import KMeans

wcss = []
for i in range(1, 11):
    km = KMeans(n_clusters=i, random_state=42)
    km.fit(clustering_features)
    wcss.append(km.inertia_)

plt.plot(range(1, 11), wcss, marker='o')
plt.xlabel('Number of Clusters (K)')
plt.ylabel('WCSS (Inertia)')
plt.title('Elbow Method for Optimal K')
plt.grid(True)
plt.show()

"""**Explanation:**

* The Elbow method is used to determine the optimal number of clusters for K-Means clustering.
* This is done by by calculating the Within-Cluster Sum of Squares (WCSS) for a range of cluster numbers (1 to 10).

* The Elbow point (K=3) in the plot identifies the optimal number of clusters, which is where the decrease in WCSS starts to level off, suggesting that adding more clusters beyond this point does not significantly improve the clustering.
"""

kmeans = KMeans(n_clusters=3, random_state=42)
df['kmeans_cluster'] = kmeans.fit_predict(clustering_features)

sns.countplot(data=df, x='kmeans_cluster', palette='Set3')
plt.title('Number of Learners in Each Cluster')
plt.xlabel('Cluster')
plt.ylabel('Count')
plt.tight_layout()
plt.show()

"""**Insights:**
* It is observed that among the 3 clusters, cluster-1 consists of the highest number of alike learners (~81000).
"""

from sklearn.metrics import silhouette_score

score = silhouette_score(clustering_features, df['kmeans_cluster'])
print(f'Silhouette Score: {score:.4f}')

"""Insights:
* A Silhouette Score of 0.6107 indicates well-defined clusters, separating learners into distinct groups, but some overlap may still exist between clusters.

* The separability can be further enhanced using Principal Component Analysis. This technique reduces the dimensionality and removes noise or redundant features, making the data more compact and structured.

**Improving Clustering by using PCA:**
"""

from sklearn.decomposition import PCA

pca = PCA(n_components=2)
clustering_pca = pca.fit_transform(clustering_scaled)
kmeans = KMeans(n_clusters=3, random_state=42)
labels = kmeans.fit_predict(clustering_pca)
score = silhouette_score(clustering_pca, labels)

print(f'Improved Silhouette Score (PCA): {score:.4f}')

print("Explained Variance Ratio by PCA:", pca.explained_variance_ratio_)

plt.figure(figsize=(8,6))
plt.scatter(clustering_pca[:, 0], clustering_pca[:, 1], c=labels, cmap='viridis', alpha=0.6)
plt.title('Cluster Visualization (PCA)')
plt.xlabel('PCA 1')
plt.ylabel('PCA 2')
plt.colorbar(label='Cluster')
plt.grid(True)
plt.show()

df['kmeans_cluster'].value_counts()

"""**Insights:**
* The improved Silhouette Score of 0.9832 indicates exceptionally well-defined and well-separated clusters, suggesting that PCA has significantly enhanced clustering quality.

* The first two principal components explain 50.1% and 49.9% of the variance respectively, showing that PCA has efficiently compressed the dataset with minimal information loss.

* The clusters are fairly balanced in size—Cluster 1 (83,315), Cluster 0 (61,373), Cluster 2 (61,121) indicating that the segmentation is equitable across learners.
"""

cluster_summary = df.groupby('kmeans_cluster').agg({
    'ctc': ['mean', 'median', 'max', 'min'],
    'years_experience': ['mean', 'median'],
    'company_hash': pd.Series.mode
})
cluster_summary

"""**Insights:**

*  All three clusters have a similar average CTC (2.2-2.3 million INR).
* Each cluster is dominated by a different company_hash, suggesting that companies tend to have distinct salary and experience groupings, supporting the relevance of cluster differentiation.
"""

for cluster in sorted(df['kmeans_cluster'].unique()):
    print(f"\nTop job positions in Cluster {cluster}:")
    print(df[df['kmeans_cluster'] == cluster]['job_position'].value_counts().head(5))

"""**Insights:**
* Backend Engineer dominates across all clusters, making up the largest portion in each cluster.
* Fullstack Engineers consistently rank second in all clusters, showing strong demand and versatility across companies and experience levels.

### **Hierarchical Clustering:**
"""

from scipy.cluster.hierarchy import linkage, dendrogram, fcluster
from sklearn.preprocessing import StandardScaler

sample_df = df[['ctc', 'years_experience']].dropna().sample(n=1000, random_state=42)
scaler = StandardScaler()
sample_scaled = scaler.fit_transform(sample_df)

linked = linkage(sample_scaled, method='ward')

plt.figure(figsize=(12, 6))
dendrogram(linked, truncate_mode='lastp', p=30, leaf_rotation=90., leaf_font_size=10., show_contracted=True)
plt.title('Hierarchical Clustering Dendrogram (Truncated)')
plt.xlabel('Sample Index or (Cluster Size)')
plt.ylabel('Distance')
plt.tight_layout()
plt.show()

sample_df['h_cluster'] = fcluster(linked, t=3, criterion='maxclust')

"""**Insights:**
* The dendrogram shows a clear separation into 3 broad clusters (based on the large vertical linkage distances), which aligns with the KMeans elbow result observed previously.

* One cluster (far left) contains a significantly higher number of points  (724), suggesting a group of learners with very similar CTC and experience.

* Clusters with smaller sizes (2-5) represent the outliers, possibly top earners or freshers with unusually high or low CTCs.

* The large vertical distance between cluster merges (~35–40 height) suggests high dissimilarity between clusters, validating meaningful grouping.

**Cluster Profile Summary:**
"""

cluster_summary = df.groupby('kmeans_cluster').agg({
    'ctc': ['count', 'mean', 'median', 'min', 'max'],
    'years_experience': ['mean', 'median', 'min', 'max'],
    'job_position': lambda x: x.mode()[0] if not x.mode().empty else 'Unknown',
    'company_hash': pd.Series.nunique
})

cluster_summary.columns = ['_'.join(col).strip() for col in cluster_summary.columns.values]
cluster_summary.reset_index(inplace=True)

cluster_summary.rename(columns={
    'kmeans_cluster': 'Cluster',
    'ctc_count': 'Learner Count',
    'ctc_mean': 'Avg CTC (in millions)',
    'ctc_median': 'Median CTC (in millions)',
    'ctc_min': 'Min CTC (in millions)',
    'ctc_max': 'Max CTC (in millions)',
    'years_experience_mean': 'Avg Experience (in years)',
    'years_experience_median': 'Median Experience (in years)',
    'years_experience_min': 'Min Experience (in years)',
    'years_experience_max': 'Max Experience (in years)',
    'job_position_<lambda>': 'Most Common Job Position',
    'company_hash_nunique': 'Unique Companies'
}, inplace=True)

cluster_summary

"""**Insights:**

* Cluster 2 has the highest average CTC (~₹2.32M) and median CTC (₹900K), suggesting it contains higher-paying roles and possibly more mature companies.

* Cluster 1 has the largest learner base (83,376) and the highest number of unique companies (12,988), indicating broader company representation and possibly more varied roles.

### **Actionable Insights & Recommendations:**

* **Broaden Learner Profiles**
  * Median years of experience remains around 9 years across clusters, suggesting largely mid-level professionals enroll at Scaler.
  * Custom learning paths are to be developed for freshers and senior engineers, to expand user base.

* **Missing and Generic Job Titles:**

   * A significant portion of learners had "Unknown" or "Other" as job titles before imputation, indicating poor data entry.
   *  Needs better job title standardization during onboarding and user profiling.

* **Backend Engineer Dominance:**

   * Across all clusters, Backend Engineer is the most common role, indicating high learner interest and demand in this area.

   * Scaler should also enrich content other areas of learner interest, to attract more learners, while also maintaining quality learning in Web Development.

* **Targeted Job Assistance:**

  * Use clustering to match learners with jobs from similar CTC/experience tiers.

  * Introduce a job recommendation engine based on cluster traits

* **Upskill Low-Earning Learners:**

  * Although cluster-1 has the largest count but it also has the lowest avg CTC.
  * Additional assistance and interview prep may be required for such learners, inorder to enhance their career prospectives.

* **Strengthen Alumni Relations & Branding:**

  * Highlight high-CTC earners and top job switchers as those from Cluster 2 as success stories in marketing and outreach.

In conclusion, leveraging data-driven clustering and actionable insights enables Scaler to strategically enhance its offerings, personalize learner journeys and position itself as a premier tech-education platform that not only educates but also accelerates meaningful career outcomes.
"""